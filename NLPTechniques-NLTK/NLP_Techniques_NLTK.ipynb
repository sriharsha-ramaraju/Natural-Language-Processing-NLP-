{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing different NLP Tasks such as tokenization,stemmming, lemmatisation, removal of stopwords, \n",
    "#pos tagging etc will be implemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey How are you?',\n",
       " \"How's NLP going?\",\n",
       " \"Isn't it cool to extract patterns from text!\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenisation: process of getting sentences out of list of sentences or document, or getting words out of sentences\n",
    "\n",
    "samp_text=\"Hey How are you? How's NLP going? Isn't it cool to extract patterns from text!\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sent_tokenize(samp_text)#extracting sentences from list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'How',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'going',\n",
       " '?',\n",
       " 'Is',\n",
       " \"n't\",\n",
       " 'it',\n",
       " 'cool',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'patterns',\n",
       " 'from',\n",
       " 'text',\n",
       " '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(samp_text)#extracting sentences from list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decreas'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming-one of the normalisation techniques, which reduces the words to their base forms by removing influctions\n",
    "\n",
    "from nltk.stem import PorterStemmer #porter stemmer is frequently used\n",
    "\n",
    "stemmer=PorterStemmer() #creating an object of that porter stemmer class\n",
    "\n",
    "stemmer.stem('playing')\n",
    "stemmer.stem('played')\n",
    "stemmer.stem('decreases')#stems to a word \"decreas\" which is not in the dictionary at all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increase'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as stemming sometimes gives words that are not present in dictionary, we have to use lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm=WordNetLemmatizer()\n",
    "lemm.lemmatize('increases') #now the root word is much cleaner when compared to stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm.lemmatize('running') #converted to running, because part of speech tag is not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'NNP'),\n",
       " ('How', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('?', '.'),\n",
       " ('How', 'WRB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('NLP', 'NNP'),\n",
       " ('going', 'VBG'),\n",
       " ('?', '.'),\n",
       " ('Is', 'VBZ'),\n",
       " (\"n't\", 'RB'),\n",
       " ('it', 'PRP'),\n",
       " ('cool', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('extract', 'VB'),\n",
       " ('patterns', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm.lemmatize('running',pos='v')#outputs run\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger') if pos_tags doesnt work, execute this\n",
    "from nltk import pos_tag\n",
    "pos_tag(word_tokenize(samp_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.n.01'),\n",
       " Synset('good.n.02'),\n",
       " Synset('good.n.03'),\n",
       " Synset('commodity.n.01'),\n",
       " Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21'),\n",
       " Synset('well.r.01'),\n",
       " Synset('thoroughly.r.02')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk has connection with wordnet, which is comprehensive vocabulary of all possible words\n",
    "#so you can get synonyms and anotonyms\n",
    "#pstag=pos_tag(word_tokenize(samp_text))\n",
    "#lemm.lemmatize(samp_text,pos=pstag)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hey', 'How', 'are')\n",
      "('How', 'are', 'you')\n",
      "('are', 'you', '?')\n",
      "('you', '?', 'How')\n",
      "('?', 'How', \"'s\")\n",
      "('How', \"'s\", 'NLP')\n",
      "(\"'s\", 'NLP', 'going')\n",
      "('NLP', 'going', '?')\n",
      "('going', '?', 'Is')\n",
      "('?', 'Is', \"n't\")\n",
      "('Is', \"n't\", 'it')\n",
      "(\"n't\", 'it', 'cool')\n",
      "('it', 'cool', 'to')\n",
      "('cool', 'to', 'extract')\n",
      "('to', 'extract', 'patterns')\n",
      "('extract', 'patterns', 'from')\n",
      "('patterns', 'from', 'text')\n",
      "('from', 'text', '!')\n"
     ]
    }
   ],
   "source": [
    "#creating n-grams\n",
    "from nltk import ngrams\n",
    "ngrams(word_tokenize(samp_text),3)#output is generator..so we should use for loop to see the output\n",
    "\n",
    "for grm in ngrams(word_tokenize(samp_text),3): #trigrams, since n=3\n",
    "    print(grm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "Hey David, how are you? Are you going to IBM Corp tomorrow?\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "s1='Hey David, how are you? Are you going to IBM Corp tomorrow?'\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey David, how are you? Are you going to IBM Corp tomorrow?\n"
     ]
    }
   ],
   "source": [
    "s1='Hey David, how are you? Are you going to IBM Corp tomorrow?'\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "tokens=nltk.word_tokenize(s1)#need to tokenize\n",
    "pos=nltk.pos_tag(tokens)#get pos tags for tokens\n",
    "chunks=nltk.ne_chunk(pos)#get named entity chunks from parts of speech tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Hey/NNP)\n",
      "(PERSON David/NNP)\n",
      "(',', ',')\n",
      "('how', 'WRB')\n",
      "('are', 'VBP')\n",
      "('you', 'PRP')\n",
      "('?', '.')\n",
      "('Are', 'VBP')\n",
      "('you', 'PRP')\n",
      "('going', 'VBG')\n",
      "('to', 'TO')\n",
      "(ORGANIZATION IBM/NNP Corp/NNP)\n",
      "('tomorrow', 'NN')\n",
      "('?', '.')\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:#printing all chunks\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Hey/NNP)\n",
      "(PERSON David/NNP)\n",
      "(ORGANIZATION IBM/NNP Corp/NNP)\n"
     ]
    }
   ],
   "source": [
    "#but of all chunks printed earlier, only few of them are named entities\n",
    "\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "       print(chunk) #below words are named entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
